{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f99872",
   "metadata": {},
   "source": [
    "# Preparación de Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885d9bfb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PASO 1: PREPARAR ENTORNO Y CARGAR DATOS\n",
      "==================================================\n",
      "spark://spark-master:7077\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o27.applyModifiableSettings.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.SparkSession$.conf$lzycompute$1(SparkSession.scala:1213)\n\tat org.apache.spark.sql.SparkSession$.conf$1(SparkSession.scala:1213)\n\tat org.apache.spark.sql.SparkSession$.applyModifiableSettings(SparkSession.scala:1216)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     14\u001b[39m master_url = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mSPARK_MASTER_URL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlocal[*]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(master_url)\n\u001b[32m     17\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMiAplicacionSpark\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_url\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSesión de Spark creada exitosamente.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# c. Cargar el archivo CSV en un DataFrame\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Asumimos que el archivo está en la ruta mapeada por el volumen\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/sql/session.py:504\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    500\u001b[39m     session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSparkSession$\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMODULE$\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapplyModifiableSettings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o27.applyModifiableSettings.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.SparkSession$.conf$lzycompute$1(SparkSession.scala:1213)\n\tat org.apache.spark.sql.SparkSession$.conf$1(SparkSession.scala:1213)\n\tat org.apache.spark.sql.SparkSession$.applyModifiableSettings(SparkSession.scala:1216)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "# procesamiento_etl.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, desc\n",
    "from pyspark.sql.types import FloatType\n",
    "import os\n",
    "\n",
    "# --- 1. Preparar el entorno y cargar datos ---\n",
    "print(\"==================================================\")\n",
    "print(\"PASO 1: PREPARAR ENTORNO Y CARGAR DATOS\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# Leer la URL del master desde la variable de entorno\n",
    "# master_url = os.getenv(\"SPARK_MASTER_URL\", \"local[*]\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EcommerceAnalysis\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4041\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.master\n",
    "spark.sparkContext.uiWebUrl\n",
    "\n",
    "print(\"Sesión de Spark creada exitosamente.\")\n",
    "\n",
    "# c. Cargar el archivo CSV en un DataFrame\n",
    "# Asumimos que el archivo está en la ruta mapeada por el volumen\n",
    "file_path = \"/opt/spark/data/e-commerce_orders.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df = spark.read.option(\"delimiter\", \";\").csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Archivo '{file_path}' cargado.\")\n",
    "print(f\"Esquema inferido:\")\n",
    "df.printSchema()\n",
    "print(\"\\nPrimeras 5 filas del DataFrame original:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82787a1",
   "metadata": {},
   "source": [
    "# Exploración y Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea83ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. Exploración y limpieza ---\n",
    "print(\"\\n\\n==================================================\")\n",
    "print(\"PASO 2: EXPLORACIÓN Y LIMPIEZA\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# a. Visualizar las primeras filas (ya hecho arriba, pero podemos repetirlo para seguir la estructura)\n",
    "print(\"Visualizando las primeras filas para exploración inicial:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ebcfa",
   "metadata": {},
   "source": [
    "# validar y Eliminar Registros Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988df061",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# b. Validar y eliminar registros nulos\n",
    "print(\"Contando registros nulos por columna antes de la limpieza:\")\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Eliminar filas con cualquier valor nulo\n",
    "df_clean = df.na.drop()\n",
    "print(f\"Registros antes de la limpieza: {df.count()}\")\n",
    "print(f\"Registros después de eliminar nulos: {df_clean.count()}\")\n",
    "\n",
    "# c. Asegurar que los tipos de dato sean correctos\n",
    "# Vamos a forzar el tipo de 'price' a FloatType para asegurar la precisión\n",
    "df_clean = df_clean.withColumn(\"price\", col(\"price\").cast(FloatType()))\n",
    "print(\"Asegurando que la columna 'price' sea de tipo FloatType.\")\n",
    "print(\"Nuevo esquema:\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a72bdb",
   "metadata": {},
   "source": [
    "# Transformación y Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ab9ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. Transformaciones y métricas ---\n",
    "print(\"\\n\\n==================================================\")\n",
    "print(\"PASO 3: TRANSFORMACIONES Y MÉTRICAS\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# a. Calcular el monto total y promedio de ventas\n",
    "sales_metrics_df = df_clean.agg(\n",
    "    sum(\"price\").alias(\"Monto_Total_Ventas\"),\n",
    "    avg(\"price\").alias(\"Precio_Promedio_Venta\")\n",
    ")\n",
    "\n",
    "# Mostrar en consola\n",
    "sales_metrics_row = sales_metrics_df.collect()[0]\n",
    "print(\"--- Métricas Generales de Ventas ---\")\n",
    "print(f\"Monto Total de Ventas: {sales_metrics_row['Monto_Total_Ventas']:.2f}\")\n",
    "print(f\"Precio Promedio de Venta: {sales_metrics_row['Precio_Promedio_Venta']:.2f}\")\n",
    "\n",
    "\n",
    "# b. Contar la cantidad de pedidos por estado\n",
    "print(\"\\n--- Cantidad de Pedidos por Estado ---\")\n",
    "orders_by_status = df_clean.groupBy(\"Status\").count().orderBy(desc(\"count\"))\n",
    "orders_by_status.show()\n",
    "\n",
    "\n",
    "# c. Obtener el top 3 de clientes que más dinero gastaron\n",
    "print(\"\\n--- Top 3 Clientes por Gasto Total ---\")\n",
    "top_customers = df_clean.groupBy(\"customer_id\") \\\n",
    "    .agg(sum(\"price\").alias(\"Total_Gastado\")) \\\n",
    "    .orderBy(desc(\"Total_Gastado\")) \\\n",
    "    .limit(3)\n",
    "top_customers.show()\n",
    "\n",
    "# Opcional: Guardar los resultados en CSV como ya lo hacías\n",
    "# Esto demuestra la parte \"Load\" del ETL\n",
    "print(\"\\n\\n==================================================\")\n",
    "print(\"PASO 4: CARGA DE RESULTADOS (LOAD)\")\n",
    "print(\"==================================================\")\n",
    "print(\"Guardando resultados en el directorio /opt/spark/data/resultados/...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633bec65-9153-487b-a961-3bce6eccbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escritura en CSV\n",
    "\n",
    "sales_metrics_df.write.csv(\"/opt/spark/data/resultados/monto_promedio\", mode=\"overwrite\", header=True)\n",
    "orders_by_status.write.csv(\"/opt/spark/data/resultados/pedidos_por_estado\", mode=\"overwrite\", header=True)\n",
    "top_customers.write.csv(\"/opt/spark/data/resultados/top_clientes\", mode=\"overwrite\", header=True)\n",
    "# Spark divide el dataset en particiones pero con coalesce reduce a una partición. No es recomendado en Spark porque los dataframes son generalmente cientos de veces mas grandes y afecta el rendimiento\n",
    "df_clean.coalesce(1).write.csv(\"/opt/spark/data/resultados/limpio\", mode=\"overwrite\", header=True)\n",
    "\n",
    "print(\"Proceso ETL completado exitosamente.\")\n",
    "\n",
    "# NO necesitas spark.stop() por la corrección que ya aplicaste."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
