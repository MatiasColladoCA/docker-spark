{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f99872",
   "metadata": {},
   "source": [
    "# Preparación de Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885d9bfb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PASO 1: PREPARAR ENTORNO Y CARGAR DATOS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/29 00:46:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesión de Spark creada exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo '/opt/spark/data/e-commerce_orders.csv' cargado.\n",
      "Esquema inferido:\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_approved_at: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- shipping_charges: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "\n",
      "Primeras 5 filas del DataFrame original:\n",
      "+------------+------------+------------------------+-----------------+------+----------------+---------+\n",
      "|    order_id| customer_id|order_purchase_timestamp|order_approved_at| price|shipping_charges|   status|\n",
      "+------------+------------+------------------------+-----------------+------+----------------+---------+\n",
      "|u6rPMRAYIGig|I74lXDOfoqsp|              18/11/2017|       18/11/2017|   241|             209|EJECUTADO|\n",
      "|ohY8f4FEbX19|47TuLHF2s7X5|                2/6/2018|         2/6/2018|  4289|            1228|EJECUTADO|\n",
      "|I28liQek73i2|dQ0dqI8Qwlj8|                8/1/2018|         9/1/2018|  5021|            6711|EJECUTADO|\n",
      "|bBG1T89mlY8W|iQCmWhNkIczb|               10/3/2017|        10/3/2017|   891|            6205|EJECUTADO|\n",
      "|CYxJJSQS8Lbo|Dp2g6JH8tO5Z|               2/12/2017|        5/12/2017|213999|             941|PENDIENTE|\n",
      "+------------+------------+------------------------+-----------------+------+----------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# procesamiento_etl.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, desc\n",
    "from pyspark.sql.types import FloatType\n",
    "import os\n",
    "\n",
    "# --- 1. Preparar el entorno y cargar datos ---\n",
    "print(\"==================================================\")\n",
    "print(\"PASO 1: PREPARAR ENTORNO Y CARGAR DATOS\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# Leer la URL del master desde la variable de entorno\n",
    "# master_url = os.getenv(\"SPARK_MASTER_URL\", \"local[*]\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EcommerceAnalysis\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.ui.port\", \"4041\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.master\n",
    "spark.sparkContext.uiWebUrl\n",
    "\n",
    "print(\"Sesión de Spark creada exitosamente.\")\n",
    "\n",
    "# c. Cargar el archivo CSV en un DataFrame\n",
    "# Asumimos que el archivo está en la ruta mapeada por el volumen\n",
    "file_path = \"/opt/spark/data/e-commerce_orders.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "df = spark.read.option(\"delimiter\", \";\").csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Archivo '{file_path}' cargado.\")\n",
    "print(f\"Esquema inferido:\")\n",
    "df.printSchema()\n",
    "print(\"\\nPrimeras 5 filas del DataFrame original:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82787a1",
   "metadata": {},
   "source": [
    "# Exploración y Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ea83ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "PASO 2: EXPLORACIÓN Y LIMPIEZA\n",
      "==================================================\n",
      "Visualizando las primeras filas para exploración inicial:\n",
      "+------------+------------+------------------------+-----------------+------+----------------+---------+\n",
      "|    order_id| customer_id|order_purchase_timestamp|order_approved_at| price|shipping_charges|   status|\n",
      "+------------+------------+------------------------+-----------------+------+----------------+---------+\n",
      "|u6rPMRAYIGig|I74lXDOfoqsp|              18/11/2017|       18/11/2017|   241|             209|EJECUTADO|\n",
      "|ohY8f4FEbX19|47TuLHF2s7X5|                2/6/2018|         2/6/2018|  4289|            1228|EJECUTADO|\n",
      "|I28liQek73i2|dQ0dqI8Qwlj8|                8/1/2018|         9/1/2018|  5021|            6711|EJECUTADO|\n",
      "|bBG1T89mlY8W|iQCmWhNkIczb|               10/3/2017|        10/3/2017|   891|            6205|EJECUTADO|\n",
      "|CYxJJSQS8Lbo|Dp2g6JH8tO5Z|               2/12/2017|        5/12/2017|213999|             941|PENDIENTE|\n",
      "+------------+------------+------------------------+-----------------+------+----------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Exploración y limpieza ---\n",
    "print(\"\\n\\n==================================================\")\n",
    "print(\"PASO 2: EXPLORACIÓN Y LIMPIEZA\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# a. Visualizar las primeras filas (ya hecho arriba, pero podemos repetirlo para seguir la estructura)\n",
    "print(\"Visualizando las primeras filas para exploración inicial:\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ebcfa",
   "metadata": {},
   "source": [
    "# validar y Eliminar Registros Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988df061",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contando registros nulos por columna antes de la limpieza:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------------------+-----------------+-----+----------------+------+\n",
      "|order_id|customer_id|order_purchase_timestamp|order_approved_at|price|shipping_charges|status|\n",
      "+--------+-----------+------------------------+-----------------+-----+----------------+------+\n",
      "|       0|          0|                       0|               36|    0|               0|     0|\n",
      "+--------+-----------+------------------------+-----------------+-----+----------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros antes de la limpieza: 229674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==============>                                           (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros después de eliminar nulos: 229638\n",
      "Asegurando que la columna 'price' sea de tipo FloatType.\n",
      "Nuevo esquema:\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_approved_at: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- shipping_charges: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# b. Validar y eliminar registros nulos\n",
    "print(\"Contando registros nulos por columna antes de la limpieza:\")\n",
    "from pyspark.sql.functions import isnan, when, count\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Eliminar filas con cualquier valor nulo\n",
    "df_clean = df.na.drop()\n",
    "print(f\"Registros antes de la limpieza: {df.count()}\")\n",
    "print(f\"Registros después de eliminar nulos: {df_clean.count()}\")\n",
    "\n",
    "# c. Asegurar que los tipos de dato sean correctos\n",
    "# Vamos a forzar el tipo de 'price' a FloatType para asegurar la precisión\n",
    "df_clean = df_clean.withColumn(\"price\", col(\"price\").cast(FloatType()))\n",
    "print(\"Asegurando que la columna 'price' sea de tipo FloatType.\")\n",
    "print(\"Nuevo esquema:\")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a72bdb",
   "metadata": {},
   "source": [
    "# Transformación y Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9ab9ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "PASO 3: TRANSFORMACIONES Y MÉTRICAS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Métricas Generales de Ventas ---\n",
      "Monto Total de Ventas: 3688375824.00\n",
      "Precio Promedio de Venta: 16061.70\n",
      "\n",
      "--- Cantidad de Pedidos por Estado ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|    Status| count|\n",
      "+----------+------+\n",
      "| EJECUTADO|198240|\n",
      "| PENDIENTE| 27006|\n",
      "|SUSPENDIDO|  4392|\n",
      "+----------+------+\n",
      "\n",
      "\n",
      "--- Top 3 Clientes por Gasto Total ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "| customer_id|Total_Gastado|\n",
      "+------------+-------------+\n",
      "|XF9C78TLFvuT|    2459994.0|\n",
      "|pLsmwxaiSOTf|    2459994.0|\n",
      "|v2q09pZEMFj4|    2459994.0|\n",
      "+------------+-------------+\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "PASO 4: CARGA DE RESULTADOS (LOAD)\n",
      "==================================================\n",
      "Guardando resultados en el directorio /opt/spark/data/resultados/...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Transformaciones y métricas ---\n",
    "print(\"\\n\\n==================================================\")\n",
    "print(\"PASO 3: TRANSFORMACIONES Y MÉTRICAS\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "# a. Calcular el monto total y promedio de ventas\n",
    "sales_metrics_df = df_clean.agg(\n",
    "    sum(\"price\").alias(\"Monto_Total_Ventas\"),\n",
    "    avg(\"price\").alias(\"Precio_Promedio_Venta\")\n",
    ")\n",
    "\n",
    "# Mostrar en consola\n",
    "sales_metrics_row = sales_metrics_df.collect()[0]\n",
    "print(\"--- Métricas Generales de Ventas ---\")\n",
    "print(f\"Monto Total de Ventas: {sales_metrics_row['Monto_Total_Ventas']:.2f}\")\n",
    "print(f\"Precio Promedio de Venta: {sales_metrics_row['Precio_Promedio_Venta']:.2f}\")\n",
    "\n",
    "\n",
    "# b. Contar la cantidad de pedidos por estado\n",
    "print(\"\\n--- Cantidad de Pedidos por Estado ---\")\n",
    "orders_by_status = df_clean.groupBy(\"Status\").count().orderBy(desc(\"count\"))\n",
    "orders_by_status.show()\n",
    "\n",
    "\n",
    "# c. Obtener el top 3 de clientes que más dinero gastaron\n",
    "print(\"\\n--- Top 3 Clientes por Gasto Total ---\")\n",
    "top_customers = df_clean.groupBy(\"customer_id\") \\\n",
    "    .agg(sum(\"price\").alias(\"Total_Gastado\")) \\\n",
    "    .orderBy(desc(\"Total_Gastado\")) \\\n",
    "    .limit(3)\n",
    "top_customers.show()\n",
    "\n",
    "# Opcional: Guardar los resultados en CSV como ya lo hacías\n",
    "# Esto demuestra la parte \"Load\" del ETL\n",
    "print(\"\\n\\n==================================================\")\n",
    "print(\"PASO 4: CARGA DE RESULTADOS (LOAD)\")\n",
    "print(\"==================================================\")\n",
    "print(\"Guardando resultados en el directorio /opt/spark/data/resultados/...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633bec65-9153-487b-a961-3bce6eccbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escritura en CSV\n",
    "\n",
    "# sales_metrics_df.write.csv(\"/opt/spark/data/resultados/monto_promedio\", mode=\"overwrite\", header=True)\n",
    "# orders_by_status.write.csv(\"/opt/spark/data/resultados/pedidos_por_estado\", mode=\"overwrite\", header=True)\n",
    "# top_customers.write.csv(\"/opt/spark/data/resultados/top_clientes\", mode=\"overwrite\", header=True)\n",
    "# # Spark divide el dataset en particiones pero con coalesce reduce a una partición. No es recomendado en Spark porque los dataframes son generalmente cientos de veces mas grandes y afecta el rendimiento\n",
    "# df_clean.coalesce(1).write.csv(\"/opt/spark/data/resultados/limpio\", mode=\"overwrite\", header=True)\n",
    "\n",
    "# print(\"Proceso ETL completado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "250f61f9-90fc-4352-b58c-f4a09e21cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cerramos explícitamente la sesión\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
