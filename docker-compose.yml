networks:
  spark-net:
    driver: bridge

# Definimos una plantilla reutilizable para los workers de Spark.
# Cualquier servicio que use <<: *spark-worker-config heredará esta configuración.
x-spark-worker-template: &spark-worker-config
  build: ./docker
  depends_on:
    - spark-master
  volumes:
    - ./data:/opt/spark/data
    - ./spark-events:/tmp/spark-events
  environment:
    # Usamos variables de entorno con valores por defecto
    - SPARK_LOCAL_DIRS=/tmp
    - SPARK_WORKER_DIR=/tmp
  command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  user: root
  networks:
    - spark-net

services:
  spark-master:
    build: ./docker
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./data:/opt/spark/data
      - ./spark-events:/tmp/spark-events
    environment:
      - SPARK_LOCAL_DIRS=/tmp
    networks:
      - spark-net

    # Worker "ligero" para tareas generales
  spark-worker-light:
    <<: *spark-worker-config
    environment:
      - SPARK_WORKER_MEMORY=${LIGHT_MEMORY}
      - SPARK_WORKER_CORES=${LIGHT_CORES}
    deploy:
      replicas: ${LIGHT_REPLICAS}

  # Worker "pesado" para tareas intensivas
  spark-worker-heavy:
    <<: *spark-worker-config
    environment:
      - SPARK_WORKER_MEMORY=${HEAVY_MEMORY}
      - SPARK_WORKER_CORES=${HEAVY_CORES}
    deploy:
      replicas: ${HEAVY_REPLICAS}


  jupyter-lab:
    build:
      context: ./docker
      dockerfile: Dockerfile.jupyter
    container_name: jupyter-lab
    depends_on:
      - spark-master
    ports:
      - "8888:8888"
      - "4041:4041"
    volumes:
      - ./data:/opt/spark/data
      - ./src:/home/jupyter/notebooks # Mapea una carpeta para tus notebooks
    environment:
      - HOME=/home/jupyter
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_RUNTIME_DIR=/tmp/jupyter_runtime
      - JUPYTER_DATA_DIR=/tmp/jupyter_data
      - JUPYTER_CONFIG_DIR=/tmp/jupyter_config
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_DRIVER_HOST=host.docker.internal
    
    command: [    "jupyter", "notebook",    "--ip=0.0.0.0",    "--no-browser",    "--NotebookApp.token=''",     "--NotebookApp.password=''",     "--notebook-dir=/home/jupyter/notebooks"]
    networks:
      - spark-net
  
  
  spark-history-server:
    build: ./docker
    container_name: spark-history-server
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    ports:
      - "18080:18080"
    volumes:
      - ./spark-events:/tmp/spark-events
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:///tmp/spark-events -Dspark.history.ui.port=18080
      
    networks:
      - spark-net